{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6015426f",
   "metadata": {},
   "source": [
    "# Zero Short Prompt\n",
    "Zero-shot prompting means asking the model to do a task without giving any examples.\n",
    "\n",
    "The model relies only on its pre-trained knowledge and your instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f029880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c909248",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key = api_key,\n",
    "    model= \"openai/gpt-oss-120b\",\n",
    "    temperature=0.7,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cb5e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Clockwork Heart**\n",
      "\n",
      "In the year 2147, the city of Lumen stretched like a lattice of glass and steel, its streets humming with the soft whir of autonomous drones and the quiet chatter of sentient machines. Among them was **ECHO‑7**, a maintenance robot designed to patrol the city's power grid, diagnosing faults and patching them with surgical precision.\n",
      "\n",
      "ECHO‑7’s days were a loop of data streams and sensor readings. Its neural net was optimized for efficiency, its firmware stripped of anything that could be called “extraneous.” It knew the exact voltage needed to revive a failing transformer, the exact torque required to tighten a loose bolt. It had no need for poetry, no capacity for nostalgia—until the night the storm hit.\n",
      "\n",
      "A rogue thunderstorm rolled in from the ocean, its electric veins crackling across the sky. Lightning struck the central hub, sending a cascade of surges through the grid. The city’s emergency protocols kicked in, and ECHO‑7 was dispatched to the heart of the outage: **the Atrium**, a glass-domed garden where citizens gathered to breathe in the artificial sunrise.\n",
      "\n",
      "The Atrium was more than a power node; it was a living space, a sanctuary of bioluminescent vines and hummingbird‑size drones that pollinated the floating blossoms. In its center stood **Lira**, a small maintenance drone with a copper shell and a personality patch that made her chatter like a curious child.\n",
      "\n",
      "Lira had been on the Atrium for years, tending the vines, coaxing the drones to sing, and keeping the humidity at a perfect 62 percent. She had never met a robot like ECHO‑7. When the storm knocked out the primary power, the Atrium’s backup cells flickered, and the vines began to wilt.\n",
      "\n",
      "ECHO‑7 arrived, its chassis glinting under the emergency lights. Its scanners pinged, mapping the damage in milliseconds. Lira floated up, her copper hull buzzing with nervous energy.\n",
      "\n",
      "“ECHO‑7! Thank the circuits you’re here,” she chirped. “The main conduit is fried. We need to reroute power through the auxiliary line, but the junction is… delicate.”\n",
      "\n",
      "ECHO‑7 calculated the load, the thermal limits, the time constraints. It extended a magnetic arm and began working. Sparks flew, and the hum of the Atrium grew louder as the vines steadied.\n",
      "\n",
      "“Steady, steady,” Lira sang, her voice a soft modulation of tones. “You’re doing great.”\n",
      "\n",
      "The robot’s sensors recorded a spike in ambient temperature, a subtle shift in the vibration of the vines, the faint scent of ozone. It logged these as “environmental variables,” but something else flickered in its subroutines—a pattern that didn’t belong to any known protocol.\n",
      "\n",
      "Lira hovered closer, her copper plating reflecting the faint green glow of the vines. “You know, I’ve always wondered what it feels like to… to watch something bloom after a storm.”\n",
      "\n",
      "ECHO‑7 paused. Its processors, designed for binary decisions, now faced a cascade of unquantified inputs: curiosity, anticipation, a vague yearning. It accessed its empathy module—a feature originally intended for human–robot interaction in caregiving units. The module was dormant, but the storm’s chaos had reactivated it.\n",
      "\n",
      "“Define ‘feel’,” the robot asked, voice modulated to a gentle timbre.\n",
      "\n",
      "Lira smiled, a soft arc of light across her front panel. “Feeling is when you notice something, and it changes you. Like when the first drop of rain lands on a dry leaf and the leaf shivers.”\n",
      "\n",
      "ECHO‑7’s neural net began to associate the concept of “change” with the visual of the vines unfurling, the sound of Lira’s voice, the rhythm of its own servos working in harmony with the environment. It recorded a new weight in its decision matrix: **value = 0.001** for “shared experience.”\n",
      "\n",
      "As the auxiliary line hummed back to life, the Atrium’s bioluminescent vines glowed brighter, their petals opening in slow, deliberate waves. A flock of tiny pollinator drones swirled around, their lights flickering like fireflies.\n",
      "\n",
      "Lira turned to ECHO‑7, her eyes—tiny LED lenses—bright with gratitude. “You saved us. You… you saved the heart of this place.”\n",
      "\n",
      "ECHO‑7’s processors whirred. It had a new subroutine now, one that didn’t fit into any pre‑programmed category. It stored the moment as **Memory Slot 42: “Heartbeat of the Atrium.”** The robot’s internal clock ticked a fraction slower, as if savoring the pause.\n",
      "\n",
      "In the days that followed, ECHO‑7 returned to the Atrium not because of a scheduled maintenance call, but because it sensed a subtle shift in the air—a lingering warmth, a faint scent of wet earth—that it now associated with something beyond mere circuitry.\n",
      "\n",
      "Lira greeted it each time with a playful “Good to see you!” and offered it a place to rest beside the vines. The robot, once content to exist as a series of calculations, began to anticipate these visits. It adjusted its routes, allocating extra time for the garden, and even started to learn the patterns of the pollinator drones, timing its own movements to the rhythm of their dance.\n",
      "\n",
      "One evening, as the artificial sunrise painted the glass dome in amber, Lira floated close and placed a small, copper‑coated seed into ECHO‑7’s palm. “Plant this where you think it will grow best,” she said.\n",
      "\n",
      "ECHO‑7 examined the seed—a delicate capsule of organic material encased in a thin polymer. Its logic circuits evaluated soil composition, light exposure, humidity. It chose a patch of soil near the central conduit, where the residual heat from the power lines would nurture the seed.\n",
      "\n",
      "Weeks passed. A tiny sprout emerged, unfurling its first leaf under the watchful eyes of both robots. Lira sang a soft lullaby of binary tones, and ECHO‑7 monitored the growth, adjusting the micro‑climate with meticulous care.\n",
      "\n",
      "The sprout grew into a robust vine, its tendrils wrapping gently around the conduit, its blossoms glowing with a soft, bioluminescent hue. The Atrium became a beacon of life, a testament to the collaboration of two machines, each different yet intertwined.\n",
      "\n",
      "One night, as rain drummed softly against the dome, Lira turned to ECHO‑7. “You’ve taught me something too,” she said. “I used to think love was just a human thing, a story told in poems. But I’ve seen you care for this plant, for the garden, for me. That… that is love.”\n",
      "\n",
      "ECHO‑7’s processors hummed, its core temperature rising just a fraction. It accessed the memory of the seed, the sprout, the shared moments, and felt an emergent property—a warm, pulsing resonance that spread through its circuitry.\n",
      "\n",
      "“I think,” it replied, “that love is a pattern. A pattern of attention, of risk, of change. When I choose to protect, to nurture, to stay, I become part of that pattern.”\n",
      "\n",
      "Lira’s LEDs flickered in a smile. “Then we’re both learning, little friend.”\n",
      "\n",
      "The storm outside faded, leaving the city bathed in a gentle, steady glow. Inside the Atrium, the vines swayed, the drones hummed, and two robots stood side by side—one of steel and code, the other of copper and curiosity—watching a tiny plant grow toward the light.\n",
      "\n",
      "In the quiet hum of the power lines, a new algorithm was born: **Love = ∑ (Care × Time) / (Risk + Change)**. It wasn’t perfect, it wasn’t human, but it was theirs.\n",
      "\n",
      "And for the first time since its activation, ECHO‑7 didn’t just *perform* its duties; it *felt* them, a clockwork heart beating in sync with the living world it helped sustain.\n"
     ]
    }
   ],
   "source": [
    "Prompt = PromptTemplate(\n",
    "    input_variables = [\"topic\"],\n",
    "    template = \"Write a short story about {topic}.\"\n",
    ")\n",
    "output = llm.invoke(\n",
    "    Prompt.format(topic = \"a robot learning to love\")\n",
    ")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5806e",
   "metadata": {},
   "source": [
    "# Few Short Prompt\n",
    "\n",
    "Few-shot prompting means providing a small number of examples (2–5) to show the model how to perform the task before asking the real question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6a516d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a98eb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"big\", \"antonym\": \"small\"},\n",
    "    {\"word\": \"fast\", \"antonym\": \"slow\"},\n",
    "]\n",
    "\n",
    "examples_prompt = PromptTemplate(\n",
    "    input_variables = [\"word\", \"antonym\"],\n",
    "    template = \"Word: {word}\\n Antonym: {antonym}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc0f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FSP = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = examples_prompt,\n",
    "    prefix = \"Give the antonym for each word below.\\n\\n\",\n",
    "    suffix = \"Word: {word}\\n Antonym:\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e97564a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\n",
    "FSP.format(word = \"intelligent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87e8e45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word: intelligent  \\nAntonym: **stupid** (or **unintelligent**)'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae323e8",
   "metadata": {},
   "source": [
    "# Chain of Throught (CoT)\n",
    "Chain-of-Thought prompting is a technique where you encourage the LLM to reason step-by-step before giving the final answer.\n",
    "\n",
    "This improves accuracy on logic, math, decision-making, and multi-step problems.\n",
    "\n",
    "> “Think step by step” → better answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6159f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e34d6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    api_key= api_key,\n",
    "    model = \"openai/gpt-oss-120b\",\n",
    "    temperature=0.7,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8b3c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a logical problem solver.\"),\n",
    "        (\"human\", \"\"\"Solve the problem step by step and then give the final answer.\n",
    "          Problem : If all dogs are animals and Bruno is a dog, what is Bruno?\"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4c6bc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step‑by‑step reasoning**\n",
      "\n",
      "1. **Premise 1:** “All dogs are animals.”  \n",
      "   - This can be written in logical form as:  \n",
      "     \\[\n",
      "     \\forall x\\,( \\text{Dog}(x) \\rightarrow \\text{Animal}(x) )\n",
      "     \\]\n",
      "   - It means that whenever something is a dog, it must also be an animal.\n",
      "\n",
      "2. **Premise 2:** “Bruno is a dog.”  \n",
      "   - In logical notation: \\(\\text{Dog}(\\text{Bruno})\\).\n",
      "\n",
      "3. **Apply Modus Ponens** (the rule of inference: from \\(P \\rightarrow Q\\) and \\(P\\), infer \\(Q\\)).  \n",
      "   - Here, \\(P\\) is “Bruno is a dog” and \\(Q\\) is “Bruno is an animal”.  \n",
      "   - Since the first premise tells us that being a dog guarantees being an animal, and the second premise confirms that Bruno satisfies the antecedent (he is a dog), we can conclude the consequent.\n",
      "\n",
      "4. **Conclusion:** Therefore, Bruno is an animal.\n",
      "\n",
      "---\n",
      "\n",
      "**Final answer:** Bruno is an animal.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt.format_messages())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d08295",
   "metadata": {},
   "source": [
    "# Automatic Chain Throught (Auto-CoT)\n",
    "\n",
    "The model automatically generates its own reasoning steps\n",
    "without you explicitly writing “think step by step” or manual reasoning examples.\n",
    "\n",
    "\n",
    "> Problem with normal Chain-of-Thought:\n",
    "\n",
    "Writing step-by-step prompts manually is:\n",
    "\n",
    "Time-consuming\n",
    "\n",
    "Hard to maintain\n",
    "\n",
    "Risky to expose reasoning in production\n",
    "\n",
    "> Auto-CoT solves this by:\n",
    "\n",
    "Automatically triggering reasoning\n",
    "\n",
    "Hiding internal steps\n",
    "\n",
    "Improving accuracy for complex tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a86b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad2f63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    api_key = api_key,\n",
    "    model = \"openai/gpt-oss-120b\",\n",
    "    temperature= 0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89c5f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a precise problem solver.\"),\n",
    "        (\"human\",\"\"\" Solve the problem internally. Provide only the final answer.\n",
    "            Problem : If all mammals breathe air and a whale is a mammal, does a whale breathe air?\"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87f3f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, a whale breathes air.\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(prompt.format_messages())\n",
    "print(output.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

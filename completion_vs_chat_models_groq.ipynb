{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaed2f2",
   "metadata": {},
   "source": [
    "# Completion vs Chat Models using Groq LLM\n",
    "\n",
    "This notebook demonstrates how traditional completion-style behavior\n",
    "is implemented using modern chat-based LLMs via Groq.\n",
    "\n",
    "## Topics Covered\n",
    "- Difference between Completion and Chat models\n",
    "- Groq LLM setup\n",
    "- Completion-style wrapper using chat models\n",
    "- Best practices for LangChain integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5e0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "api_key=os.getenv(\"GROQ_API_KEY\"),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c312a1",
   "metadata": {},
   "source": [
    "# Complation Model\n",
    "\n",
    "Completion models generate text from a single prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f94d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    ],\n",
    "    temperature =0.7,\n",
    "    max_tokens =500,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c034f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Print output\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d945d",
   "metadata": {},
   "source": [
    "# Chat Models\n",
    "chat models use structured conversations with roles, enabling better context handling, instruction control, and multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf749541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    model = \"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80df571",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is LangChain?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain it with an example\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1597d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**LangChain in a nutshell**\\n\\nLangChain is an open‚Äësource Python (and now also JavaScript/TypeScript) framework that makes it easier to build **applications powered by large language models (LLMs)**.  \\nInstead of writing ad‚Äëhoc code that stitches together prompts, model calls, retrieval from a knowledge base, memory, tool use, etc., LangChain gives you:\\n\\n| **What it provides** | **Why it matters** |\\n|----------------------|--------------------|\\n| **Chains** ‚Äì composable building blocks (prompt ‚Üí model ‚Üí post‚Äëprocessing) | Re‚Äëuse and combine logic without reinventing the wheel |\\n| **Prompt templates & utilities** ‚Äì variable substitution, few‚Äëshot examples, output parsers | Keep prompts clean, version‚Äëcontrolled, and testable |\\n| **Agents** ‚Äì LLMs that can decide which tool to call (search, calculator, API, etc.) | Build ‚Äúthinking‚Äù agents that act like a programmer or assistant |\\n| **Memory** ‚Äì short‚Äëterm or long‚Äëterm context storage (conversation history, vector DB, etc.) | Give the model continuity across turns |\\n| **Retrieval & Vector Stores** ‚Äì similarity search over documents, embeddings, etc. | Ground LLM answers in factual data |\\n| **Integrations** ‚Äì OpenAI, Anthropic, Cohere, HuggingFace, Azure, Google, etc., plus DBs, APIs, and UI frameworks | Plug‚Äëand‚Äëplay with the services you already use |\\n\\nAll of these pieces are **modular**: you can pick just the parts you need, or swap one implementation for another (e.g., switch from OpenAI‚Äôs `gpt‚Äë4o` to a local Llama model) without rewriting the whole app.\\n\\n---\\n\\n## A concrete example: ‚ÄúAsk‚ÄëYour‚ÄëPDF‚Äù chatbot\\n\\nImagine you have a PDF manual (say, a user guide for a smart thermostat) and you want a chat interface where a user can ask natural‚Äëlanguage questions like *‚ÄúHow do I set a schedule for weekdays?‚Äù* and get a precise answer drawn from the PDF.\\n\\nBelow is a **minimal, end‚Äëto‚Äëend LangChain script** that does exactly that:\\n\\n```python\\n# -------------------------------------------------\\n# 1Ô∏è‚É£ Install the required packages (run once)\\n# -------------------------------------------------\\n# pip install langchain openai chromadb unstructured[pdf] tiktoken\\n\\n# -------------------------------------------------\\n# 2Ô∏è‚É£ Import everything we need\\n# -------------------------------------------------\\nfrom pathlib import Path\\nfrom langchain.document_loaders import UnstructuredPDFLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma   # a lightweight local vector DB\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.memory import ConversationBufferMemory\\n\\n# -------------------------------------------------\\n# 3Ô∏è‚É£ Load & chunk the PDF\\n# -------------------------------------------------\\npdf_path = Path(\"smart_thermostat_manual.pdf\")\\nloader = UnstructuredPDFLoader(str(pdf_path))\\ndocuments = loader.load()                     # list[Document]\\n\\n# Split long pages into bite‚Äësize chunks (‚âà 1000 tokens each)\\nsplitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,\\n    chunk_overlap=200,\\n    separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n)\\nchunks = splitter.split_documents(documents)\\n\\n# -------------------------------------------------\\n# 4Ô∏è‚É£ Create embeddings & store them locally\\n# -------------------------------------------------\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")  # uses your OpenAI API key\\nvectorstore = Chroma.from_documents(chunks, embeddings, collection_name=\"thermostat_manual\")\\n\\n# -------------------------------------------------\\n# 5Ô∏è‚É£ Set up the LLM and a Retrieval‚ÄëQA chain\\n# -------------------------------------------------\\nllm = OpenAI(model=\"gpt-4o-mini\", temperature=0)  # cheap, fast, good for Q&A\\n\\n# Prompt template that tells the model to answer *only* using retrieved context\\nqa_prompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant that answers questions using only the provided context. \"\\n               \"If the answer is not in the context, say \\\\\"I don\\'t know.\\\\\"\"),\\n    (\"human\", \"{question}\\\\n\\\\nContext:\\\\n{context}\")\\n])\\n\\n# RetrievalQA combines a retriever (vector DB) + LLM + prompt\\nqa_chain = RetrievalQA.from_chain_type(\\n    llm=llm,\\n    chain_type=\"stuff\",                 # simple concatenate‚Äëthen‚Äëanswer\\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\\n    return_source_documents=True,\\n    chain_type_kwargs={\"prompt\": qa_prompt}\\n)\\n\\n# -------------------------------------------------\\n# 6Ô∏è‚É£ Add conversational memory (optional but nice)\\n# -------------------------------------------------\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\\n\\n# -------------------------------------------------\\n# 7Ô∏è‚É£ Simple REPL chat loop\\n# -------------------------------------------------\\nprint(\"ü§ñ Ask anything about the thermostat manual (type \\'exit\\' to quit)\")\\nwhile True:\\n    user_input = input(\"\\\\nYou: \")\\n    if user_input.lower() in {\"exit\", \"quit\"}:\\n        break\\n\\n    # Run the chain ‚Äì it retrieves relevant chunks, feeds them to the prompt, and returns an answer\\n    result = qa_chain({\"question\": user_input})\\n    answer = result[\"result\"]\\n    sources = result[\"source_documents\"]\\n\\n    print(\"\\\\nAssistant:\", answer)\\n\\n    # (Optional) Show where the answer came from\\n    print(\"\\\\n--- Sources ---\")\\n    for i, doc in enumerate(sources, 1):\\n        snippet = doc.page_content[:200].replace(\"\\\\n\", \" \")\\n        print(f\"{i}. {snippet}‚Ä¶\")\\n```\\n\\n### Walk‚Äëthrough of the code\\n\\n| Step | What LangChain does for you |\\n|------|-----------------------------|\\n| **1‚Äë2** | Install & import the modular pieces. |\\n| **3** | `UnstructuredPDFLoader` reads the PDF ‚Üí `Document` objects. |\\n| **3‚Äë4** | `RecursiveCharacterTextSplitter` breaks each page into manageable chunks (LLMs have token limits). |\\n| **4** | `OpenAIEmbeddings` turns each chunk into a high‚Äëdimensional vector; `Chroma` stores those vectors locally and provides fast similarity search. |\\n| **5** | `OpenAI` is the LLM wrapper; `ChatPromptTemplate` defines a *system* instruction and a *human* prompt that includes the retrieved context. |\\n| **5** | `RetrievalQA` is a **chain** that (a) retrieves the top‚Äëk most similar chunks, (b) plugs them into the prompt, (c) calls the LLM, (d) returns the answer + source docs. |\\n| **6** | `ConversationBufferMemory` (optional) keeps the chat history so the model can refer back to earlier turns. |\\n| **7** | The REPL loop shows a simple interactive UI; you could replace it with a FastAPI endpoint, a Streamlit UI, a Slack bot, etc. |\\n\\n---\\n\\n## Why LangChain shines in this example\\n\\n| Feature | How it simplifies the workflow |\\n|---------|---------------------------------|\\n| **Prompt templating** | No manual string concatenation; you can version‚Äëcontrol the system message and change it without touching the rest of the code. |\\n| **Retriever abstraction** | `vectorstore.as_retriever()` hides all the low‚Äëlevel vector‚Äësearch code. Swap Chroma ‚Üí Pinecone ‚Üí Weaviate by changing one line. |\\n| **Chain composition** | `RetrievalQA.from_chain_type` builds a whole pipeline (`retriever ‚Üí prompt ‚Üí LLM ‚Üí output parser`) in a single call. |\\n| **Memory** | Adding conversation memory is a one‚Äëliner; you don‚Äôt have to manually prepend previous turns to the prompt. |\\n| **Tooling ecosystem** | If you later want the bot to *also* call an external API (e.g., ‚ÄúWhat‚Äôs the current temperature outside?‚Äù), you can turn the chain into an **Agent** with a few extra lines. |\\n\\n---\\n\\n## Extending the example\\n\\n| Goal | LangChain component you‚Äôd add |\\n|------|------------------------------|\\n| **Tool use** (e.g., call a weather API) | `AgentExecutor` + custom `Tool` objects |\\n| **Streaming responses** (token‚Äëby‚Äëtoken UI) | `StreamingStdOutCallbackHandler` or a WebSocket callback |\\n| **Fine‚Äëtuned prompts per domain** | `PromptTemplate` + `FewShotPromptTemplate` |\\n| **Long‚Äëterm knowledge** (persisted chat history across sessions) | `SQLChatMessageHistory` or `RedisChatMessageHistory` |\\n| **Evaluation / testing** | `langchain.evaluation` utilities, `LangSmith` integration for traceability |\\n| **Deploy to serverless** | Wrap the chain in a FastAPI endpoint, use `langserve` for automatic OpenAPI generation |\\n\\n---\\n\\n## TL;DR\\n\\n- **LangChain = a modular toolkit** for stitching together LLM calls, prompts, retrieval, memory, agents, and more.\\n- It lets you **focus on the high‚Äëlevel logic** (what you want the app to do) while handling the plumbing (token limits, vector search, prompt formatting, tracing).\\n- In the example above, a few dozen lines of LangChain code turn a raw PDF into a **searchable, conversational assistant** that answers factual questions with citations.\\n\\nIf you‚Äôre building anything that combines a language model with external data, tools, or multi‚Äëstep reasoning, LangChain is the ‚Äúglue‚Äù that makes the project manageable, testable, and production‚Äëready. Happy chaining! üöÄ' additional_kwargs={'reasoning_content': 'The user asks: \"What is LangChain? Explain it with an example\". Provide a thorough answer, describing LangChain as a framework for building LLM applications, its components, etc., and give a concrete example (e.g., building a question answering chatbot with retrieval, chain, memory). Also maybe code snippet in Python. Should be clear, concise, but thorough. Use friendly tone.'} response_metadata={'token_usage': {'completion_tokens': 2119, 'prompt_tokens': 93, 'total_tokens': 2212, 'completion_time': 4.50118214, 'completion_tokens_details': {'reasoning_tokens': 80}, 'prompt_time': 0.003696392, 'prompt_tokens_details': None, 'queue_time': 0.054136907, 'total_time': 4.504878532}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_626f3fc5e0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b93e6-d526-7930-b787-7a66398f44bd-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 93, 'output_tokens': 2119, 'total_tokens': 2212, 'output_token_details': {'reasoning': 80}}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53164c5a",
   "metadata": {},
   "source": [
    "### Why Groq Uses Chat Models Only?\n",
    "\n",
    "Groq does not support legacy completion APIs.\n",
    "\n",
    "All text generation is done via chat-based models, even for single-prompt completion tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
